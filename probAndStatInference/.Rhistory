# Calculate the percentage of standardised scores that are greated than 1.96
# the perc function which is part of the FSA package which calculate the percentage that are within a range - you can look for greater than "gt", greater than or equal "geq", "gt", less than or equal "leq",  or less than "lt"),
# scale is a function that creates z scores
ztpstress<- abs(scale(survey$tpstress))
FSA::perc(as.numeric(ztpstress), 1.96, "gt")
FSA::perc(as.numeric(ztpstress), 3.29, "gt")
#Simple scatterplot of feeling of control and perceived stress
#aes(x,y)
scatter <- ggplot(survey, aes(tpstress, tpcoiss))
#Add a regression line
scatter + geom_point() + geom_smooth(method = "lm", colour = "Red", se = F) + labs(x = "Total Perceived Stress", y = "Total PCOISS")
#Pearson Correlation
stats::cor.test(survey$tpcoiss, survey$tpstress, method='pearson')
#If our data does not fit a normal distribution we would report either Spearman or Kurtosis
#All tests are included here for completeness of explanation - in reality you would only include the most appropriate test.
#Spearman Correlation
#Change the method to be spearman.
#This test will give an error since this method uses ranking but cannot handle ties but that is ok with us - we consider this to be missing data
cor.test(survey$tpcoiss, survey$tpstress, method = "spearman")
#We can also use kendall's tau which does handle ties
cor.test(survey$tpcoiss, survey$tpstress, method = "kendall")
needed_packages <- c("pastecs", "tidyverse", "psych", "semTools", "FSA", "car", "coin", "rstatix", "effectsize")
# Extract not installed packages
not_installed <- needed_packages[!(needed_packages %in% installed.packages()[ , "Package"])]
# Install not installed packages
if(length(not_installed)) install.packages(not_installed)
library(ggplot2) #For creating histograms with more detail than plot
library(psych) # Some useful descriptive functions
current_path = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(current_path))
#We are using a .dat file (survey.dat) created from the SPSS file survey.sav  taken from SPSS Survival Manual 6th Edition Julie Pallant
#http://spss.allenandunwin.com.s3-website-ap-southeast-2.amazonaws.com/data-files.html#.Wb0vvnWP-po
#Results on a survey on well being
#We need to load the file so that we can use it in R.
survey <- read.table("survey.dat")
#Setting the column names to be that used in the dataset
colnames(survey) <- tolower(colnames(survey))
#We will allocate the histogram to a variable to allow use to manipulate it
gg <- ggplot(survey, aes(x=tslfest))
#Change the label of the x axis
gg <- gg + labs(x="Total Self-esteem")
#manage binwidth and colours
gg <- gg + geom_histogram(binwidth=2, colour="black", aes(y=..density.., fill=..count..))
gg <- gg + scale_fill_gradient("Count", low="#DCDCDC", high="#7C7C7C")
#adding a normal curve
#use stat_function to compute a normalised score for each value of tslfest
#pass the mean and standard deviation
#use the na.rm parameter to say how missing values are handled
gg <- gg + stat_function(fun=dnorm, color="red",args=list(mean=mean(survey$tslfest, na.rm=TRUE), sd=sd(survey$tslfest, na.rm=TRUE)))
#to display the graph, request the contents of the variable be shown
gg
#Create a qqplot
qqnorm(survey$tslfest)
qqline(survey$tslfest, col=2) #show a line on theplot
#stat.desc is a function from pastecs - make sure you include the basic switch=F to ensure you don't get scienfitic notation
pastecs::stat.desc(survey$tslfest, basic=F)
#We can make our decision based on the value of the standardised score for skew and kurtosis
#We divide the skew statistic by the standard error to get the standardised score
#This will indicate if we have a problem
tpskew<-semTools::skew(survey$tslfest)
tpkurt<-semTools::kurtosis(survey$tslfest)
tpskew[1]/tpskew[2]
tpkurt[1]/tpkurt[2]
ztslfest<- abs(scale(survey$tslfest))
FSA::perc(as.numeric(ztslfest), 1.96, "gt")
FSA::perc(as.numeric(ztslfest), 3.29, "gt")
#Get descriptive stastitics by group - output as a matrix
psych::describeBy(survey$tslfest, survey$smoke, mat=TRUE)
needed_packages <- c("pastecs", "tidyverse", "psych", "semTools", "FSA", "car", "coin", "rstatix", "effectsize")
# Extract not installed packages
not_installed <- needed_packages[!(needed_packages %in% installed.packages()[ , "Package"])]
# Install not installed packages
if(length(not_installed)) install.packages(not_installed)
install.packages("psych",dependencies=TRUE)
install.packages(c("psych","GPArotation"),dependencies=TRUE)
#Load and Install required packages
install_packages <- function(pkg) {
# Install package if it is not already
if (!(pkg %in% installed.packages()[, "Package"])){
install.packages(pkg, repos='http://cran.us.r-project.org')
}
library(pkg, character.only = TRUE)
} # end installPackages()
#Create the list of packages we need
pkg_list = c("ggplot2", "dplyr", "gridExtra", "vtable", "kableExtra", "qqplotr", "psych")
#Call our function passing it the list of packages
lapply(pkg_list, install_packages)
#Get descriptive stastitics by group - output as a matrix
psych::describeBy(survey$tslfest, survey$smoke, mat=TRUE)
psych::describeBy(bike_sharing$cnt, bike_sharing$workingday, mat=TRUE)
psych::describeBy(bike_sharing$cnt, bike_sharing$workingday)
psych::describeBy(bike_sharing$cnt, bike_sharing$workingday, mat=TRUE)
psych::describeBy(bike_sharing$cnt, bike_sharing$workingday, mat=TRUE) %>%
kbl(caption = "Table 1: Fist lines of Bike Sharing Dataset") %>%
kable_styling()
psych::describeBy(bike_sharing$cnt, bike_sharing$workingday, mat=TRUE) %>%
kbl(caption = "Table 4: Descriptive statistics of bikes hired per Workingday") %>%
kable_styling()
#Conduct Levene's test for homogeneity of variance in library car - the null hypothesis is that variances in groups are equal so to assume homogeneity we woudl expect probaility to not be statistically significant.
car::leveneTest(tslfest ~ smoke, data=survey)
car::leveneTest(cnt ~ workingday, data=bike_sharing)
#Create the list of packages we need
pkg_list = c("ggplot2", "dplyr", "gridExtra", "vtable", "kableExtra", "qqplotr", "psych", "car")
#Call our function passing it the list of packages
lapply(pkg_list, install_packages)
car::leveneTest(cnt ~ workingday, data=bike_sharing)
str(bike_sharing)
bike_sharing$workingday = as.factor(bike_sharing$workingday)
car::leveneTest(cnt ~ workingday, data=bike_sharing)
workingDayLervene <- car::leveneTest(cnt ~ workingday, data=bike_sharing)
workingDayLervene$`Pr(>F)`
workingDayLervene$`Pr(>F)`[1]
workingDayLervene$`Pr(>F)`[[1]]
workingDayLervene$`Pr(>F)`[1]
workingDayLervenePvalue <- workingDayLervene$`Pr(>F)`[1]
#Conduct the t-test from package stats
#In this case we can use the var.equal = TRUE option to specify equal variances and a pooled variance estimate
stats::t.test(tslfest~smoke,var.equal=TRUE,data=survey)
res <- stats::t.test(tslfest~smoke,var.equal=TRUE,data=survey)
#Calculate Cohen's d
#artithmetically
effcd=round((2*res$statistic)/sqrt(res$parameter),2)
#Using function from effectsize package
effectsize::t_to_d(t = res$statistic, res$parameter)
#Eta squared calculation
effes=round((res$statistic*res$statistic)/((res$statistic*res$statistic)+(res$parameter)),3)
effes
res <- stats::t.test(tslfest~smoke,var.equal=TRUE,data=survey)
#Calculate Cohen's d
#artithmetically
effcd=round((2*res$statistic)/sqrt(res$parameter),2)
#Using function from effectsize package
effectsize::t_to_d(t = res$statistic, res$parameter)
#Eta squared calculation
effes=round((res$statistic*res$statistic)/((res$statistic*res$statistic)+(res$parameter)),3)
effes
res <- stats::t.test(tslfest~smoke,var.equal=TRUE,data=survey)
#Calculate Cohen's d
#artithmetically
effcd=round((2*res$statistic)/sqrt(res$parameter),2)
#Using function from effectsize package
effectsize::t_to_d(t = res$statistic, res$parameter)
#Eta squared calculation
effes=round((res$statistic*res$statistic)/((res$statistic*res$statistic)+(res$parameter)),3)
effes
res <- stats::t.test(tslfest~smoke,var.equal=TRUE,data=survey)
stats::t.test(tslfest~smoke,var.equal=TRUE,data=survey)
stats::t.test(tslfest~smoke,var.equal=FALSE,data=survey)
#Eta squared calculation
effes=round((res$statistic*res$statistic)/((res$statistic*res$statistic)+(res$parameter)),3)
effes
res <- stats::t.test(tslfest~smoke,var.equal=FALSE,data=survey)
#there is non-significant differencee on the test above so I'll skip right away to the eta calculation
#Eta squared calculation
effes=round((res$statistic*res$statistic)/((res$statistic*res$statistic)+(res$parameter)),3)
effes
#Get descriptive stastitics by group - output as a matrix
psych::describeBy(survey$tslfest, survey$smoke, mat=TRUE)
res <- stats::t.test(tslfest~smoke,var.equal=FALSE,data=survey)
#there is non-significant differencee on the test above so I'll skip right away to the eta calculation
#Eta squared calculation
effes=round((res$statistic*res$statistic)/((res$statistic*res$statistic)+(res$parameter)),3)
effes
#Get descriptive stastitics by group - output as a matrix
psych::describeBy(survey$tslfest, survey$smoke, mat=TRUE)
#Conduct Levene's test for homogeneity of variance in library car - the null hypothesis is that variances in groups are equal so to assume homogeneity we woudl expect probaility to not be statistically significant.
car::leveneTest(tslfest ~ smoke, data=survey)
#Conduct the t-test from package stats
#In this case we can use the var.equal = TRUE option to specify equal variances and a pooled variance estimate
stats::t.test(tslfest~smoke,var.equal=TRUE,data=survey)
res <- stats::t.test(tslfest~smoke,var.equal=TRUE,data=survey)
#Calculate Cohen's d
#artithmetically
effcd=round((2*res$statistic)/sqrt(res$parameter),2)
#Using function from effectsize package
effectsize::t_to_d(t = res$statistic, res$parameter)
#Eta squared calculation
effes=round((res$statistic*res$statistic)/((res$statistic*res$statistic)+(res$parameter)),3)
effes
# changes weekday to a factor so we can treat it as factor
bike_sharing$weekday = as.factor(bike_sharing$weekday)
psych::describeBy(bike_sharing$workingday, bike_sharing$weekday, mat=TRUE) %>%
kbl(caption = "Table 4: Descriptive statistics of bikes hired per Workingday") %>%
kable_styling()
bike_sharing %>% group_by(weekday,workingday)
bike_sharing %>% group_by(weekday,workingday) %>% summarise(count_days = n())
bike_sharing %>% group_by(weekday,workingday) %>% summarise(count_days = n()) %>% arrange(weekday)
bike_sharing %>% group_by(weekday,workingday) %>% summarise(count_days = n()) %>% arrange(weekday) %>%
kbl(caption = "Table 5: Count of workingdays per weekday") %>%
kable_styling()
psych::describeBy(bike_sharing$cnt, bike_sharing$workingday, mat=TRUE) %>%
kbl(caption = "Table 6: Descriptive statistics of bikes hired per weekday") %>%
kable_styling()
psych::describeBy(bike_sharing$cnt, bike_sharing$weekday, mat=TRUE) %>%
kbl(caption = "Table 6: Descriptive statistics of bikes hired per weekday") %>%
kable_styling()
#We are using a .rdata file (sleep.rdata) created from the SPSS file sleep.sav  taken from SPSS Survival Manual 6th Edition Julie Pallant
#http://spss.allenandunwin.com.s3-website-ap-southeast-2.amazonaws.com/data-files.html#.Wb0vvnWP-po
#a real data file condensed from a study conducted to explore the prevalence and impact of sleep problems on various aspects of people’s lives. Staff from a university in Melbourne, Australia were invited to complete a questionnaire containing questions about their sleep behaviour (e.g. hours slept per night), sleep problems (e.g. difficulty getting to sleep) and the impact that these problems have on aspects of their lives (work, driving, relationships). The sample consisted of 271 respondents (55% female, 45% male) ranging in age from 18 to 84 years (mean=44yrs).
current_path = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(current_path))
#We are using a .rdata file (sleep.rdata) created from the SPSS file sleep.sav  taken from SPSS Survival Manual 6th Edition Julie Pallant
#http://spss.allenandunwin.com.s3-website-ap-southeast-2.amazonaws.com/data-files.html#.Wb0vvnWP-po
#a real data file condensed from a study conducted to explore the prevalence and impact of sleep problems on various aspects of people’s lives. Staff from a university in Melbourne, Australia were invited to complete a questionnaire containing questions about their sleep behaviour (e.g. hours slept per night), sleep problems (e.g. difficulty getting to sleep) and the impact that these problems have on aspects of their lives (work, driving, relationships). The sample consisted of 271 respondents (55% female, 45% male) ranging in age from 18 to 84 years (mean=44yrs).
current_path = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(current_path))
#We need to load the file so that we can use it in R.
sleep <- read.table("sleep.rdata")
#Setting the column names to be that used in the dataset
colnames(sleep) <- tolower(colnames(sleep))
needed_packages <- c("pastecs", "ggplot2", "psych", "semTools", "FSA", "rstatix")
# Extract not installed packages
not_installed <- needed_packages[!(needed_packages %in% installed.packages()[ , "Package"])]
# Install not installed packages
if(length(not_installed)) install.packages(not_installed)
library(pastecs) #For creating descriptive statistic summaries
library(ggplot2) #For creating histograms with more detail than plot
library(psych) # Some useful descriptive functions
library(semTools) #For skewness and kurtosis
library(FSA) #For percentage
library(effectsize) #To calculate effect size for t-test
library(rstatix)#For post-hoc testing
#We will allocate the histogram to a variable to allow us to manipulate it
gg <- ggplot(sleep, aes(x=totsas))
#Change the label of the x axis
gg <- gg + labs(x="Total Total Sleepiness and associated sensation scale score")
#manage bin width and colours
gg <- gg + geom_histogram(binwidth=2, colour="black", aes(y=..density.., fill=..count..))
gg <- gg + scale_fill_gradient("Count", low="#DCDCDC", high="#7C7C7C")
#to display the graph request the contents of the variable be shown
gg
#Create a qqplot
qqnorm(sleep$totsas)
qqline(sleep$totsas, col=2) #show a line on the plot
#stat.desc is a function from pastecs - make sure you include the basic switch=F to ensure you don't get scientific notation
pastecs::stat.desc(sleep$totsas, basic=F)
#We can make our decision based on the value of the standardized score for skew and kurtosis
#We divide the skew statistic by the standard error to get the standardized score
#This will indicate if we have a problem
tpskew<-semTools::skew(sleep$totsas)
tpkurt<-semTools::kurtosis(sleep$totsas)
tpskew[1]/tpskew[2]
tpkurt[1]/tpkurt[2]
ztotsas<- abs(scale(sleep$totsas))
FSA::perc(as.numeric(ztotsas), 1.96, "gt")
FSA::perc(as.numeric(ztotsas), 3.29, "gt")
#Store the output to use in our final reporting of the outcomes of ANOVA
sasdescrip<-psych::describeBy(sleep$totsas, sleep$agegp3, mat=TRUE)
#Conduct Bartlett's test for homogeneity of variance in library car - the null hypothesis is that variances in groups are equal so to assume homogeneity we would expect probability to not be statistically significant.
stats::bartlett.test(totsas~ agegp3, data=sleep)
#Conduct Bartlett's test - the null hypothesis is that variances in groups are equal so to assume homogeneity we would expect probability to not be statistically significant.
stats::bartlett.test(totsas~ agegp3, data=sleep)
#Conduct Bartlett's test - the null hypothesis is that variances in groups are equal so to assume homogeneity we would expect probability to not be statistically significant.
stats::bartlett.test(cnt~ weekday, data=bike_sharing)
stats::bartlett.test(cnt~ weekday, data=bike_sharing)
weekdayBarlett <- stats::bartlett.test(cnt~ weekday, data=bike_sharing)
weekdayBarlett$`Pr(>F)`[1]
weekdayBarlett$p.value
# changes workingday to a factor so we can apply the Levene Test
bike_sharing$workingday = as.factor(bike_sharing$workingday)
workingDayLervene <- car::leveneTest(cnt ~ workingday, data=bike_sharing)
workingDayLervene$`Pr(>F)`[1]
#Conduct Bartlett's test - the null hypothesis is that variances in groups are equal so to assume homogeneity we would expect probability to not be statistically significant.
stats::bartlett.test(cnt~ weekday, data=bike_sharing)
weekdayBarlett <- stats::bartlett.test(cnt~ weekday, data=bike_sharing)
weekdayBarlettPvalue <- weekdayBarlett$p.value
# Compute the analysis of variance using the oneway.test from the stats package- we set VAR.equal to be true because we have homogeneity
res <- stats::oneway.test(bike_sharing$cnt ~ bike_sharing$weekday, data = bike_sharing, var.equal = TRUE)
# Summary of the analysis
res
#Compute our Eta squared
effes=effectsize::effectsize(res)
effes
# Compute the analysis of variance using the oneway.test from the stats package- we set VAR.equal to be true because we have homogeneity
res <- stats::oneway.test(bike_sharing$cnt ~ bike_sharing$weekday, data = bike_sharing, var.equal = TRUE)
# Summary of the analysis
res
#Compute our Eta squared
effes=effectsize::effectsize(res)
effes
#Store the relevant pieces of the output from ANOVA in variables to use for reporting
#Degrees of freedom
df1=res$parameter[1]
df2=res$parameter[2]
#F statistic
Fstat=round(res$statistic, 3)
#Pvalue
pval=round(res$p.value,2)
round(res$statistic, 3)
round(res$p.value,2)
round(res$statistic, 3)
res <- stats::t.test(cnt~workingday,var.equal=FALSE,data=bike_sharing)
#there is non-significant differencee on the test above so I'll skip right away to the eta calculation
#Eta squared calculation
effes=round((res$statistic*res$statistic)/((res$statistic*res$statistic)+(res$parameter)),3)
effes
res
stats::t.test(cnt~workingday,var.equal=FALSE,data=bike_sharing)
#Using the bullying.dat dataset (Paul Connolly)
#created from the .sav version available at http://cw.routledge.com/textbooks/9780415372985/resources/datasets.asp
#Contains the results of national school level survey, subset of this survey which deals with bullying.
#Question:
#"Is there a difference between boys and girls in terms of whether they have been bullied at school?"
#H0:  There is no difference
#HA: There is a difference
#Variables:
#'rsex' {1-Male, 2-Female, 99-not answered};
#'ubullsch' - Q26. Have you yourself ever been bullied in school? {1- Yes, 2 -No, 99-not answered}
bully<-read.table("bullying.dat")
needed_packages <- c("sjstats", "gmodels")
# Extract not installed packages
not_installed <- needed_packages[!(needed_packages %in% installed.packages()[ , "Package"])]
# Install not installed packages
if(length(not_installed)) install.packages(not_installed)
library(gmodels) #For creating histograms with more detail than plot
library(sjstats)#chi-square effect size
#Use the Crosstable function
#CrossTable(predictor, outcome, fisher = TRUE, chisq = TRUE, expected = TRUE)
gmodels::CrossTable(bully$rsex, bully$ubullsch, fisher = TRUE, chisq = TRUE, expected = TRUE, sresid = TRUE, format = "SPSS")
#Create your contingency table
mytable<-xtabs(~ubullsch+rsex, data=bully)
#Use the Crosstable function
#CrossTable(predictor, outcome, fisher = TRUE, chisq = TRUE, expected = TRUE)
gmodels::CrossTable(bully$rsex, bully$ubullsch, fisher = TRUE, chisq = TRUE, expected = TRUE, sresid = TRUE, format = "SPSS")
#Use the Crosstable function
#CrossTable(predictor, outcome, fisher = TRUE, chisq = TRUE, expected = TRUE)
gmodels::CrossTable(bully$rsex, bully$ubullsch, fisher = TRUE, chisq = TRUE, expected = TRUE, sresid = TRUE, format = "SPSS")
xtabs(~ubullsch+rsex, data=bully)
qplot(bike_sharing$weathersit)
qplot(geom = "barplot",bike_sharing$weathersit)
qplot(geom = "bar",bike_sharing$weathersit)
ggplot(data = bike_sharing, aes(x = weathersit)) +
geom_bar()
bike_sharing[bike_sharing$weathersit==1,]
bike_sharing[bike_sharing$weathersit==2,]
bike_sharing[bike_sharing$weathersit==3,]
bike_sharing[bike_sharing$weathersit==4,]
ggplot(data = bike_sharing, aes(x = weathersit)) +
geom_bar()
# Tranforms weathersit in factor as it's a categorical variable
bike_sharing$weathersit <- as.factor(bike_sharing$weathersit)
ggplot(data = bike_sharing, aes(x = weathersit)) +
geom_bar()
# Tranforms weathersit in factor to facilitate its usage as it's a categorical variable
bike_sharing$weathersit <- as.factor(bike_sharing$weathersit)
ggplot(data = bike_sharing, aes(x = weathersit)) +
geom_bar()
ggplot(data = bike_sharing, aes(x = weathersit), title= "aefa" +
ggplot(data = bike_sharing, aes(x = weathersit), title= "aefa") +
geom_bar()
# Tranforms weathersit in factor to facilitate its usage as it's a categorical variable
bike_sharing$weathersit <- as.factor(bike_sharing$weathersit)
ggplot(data = bike_sharing, aes(x = weathersit), title= "faf") +
geom_bar()
# Tranforms weathersit in factor to facilitate its usage as it's a categorical variable
bike_sharing$weathersit <- as.factor(bike_sharing$weathersit)
# Tranforms weathersit in factor to facilitate its usage as it's a categorical variable
bike_sharing$weathersit <- as.factor(bike_sharing$weathersit)
ggplot(data = bike_sharing, aes(x = weathersit), title= "faf") +
geom_bar()
ggplot(data = bike_sharing, aes(x = weathersit), main= "faf") +
geom_bar()
View(bike_sharing)
# Tranforms weathersit in factor to facilitate its usage as it's a categorical variable
bike_sharing$weathersit <- as.factor(bike_sharing$weathersit)
ggplot(data = bike_sharing, aes(x = weathersit), main= "faf") +
geom_bar() +
labs(x = "Weather Situation", , title = "Chart 10: Quantity of days with each weather situation")
describe(bike_sharing$weathersit)
summary(bike_sharing$weathersit)
str(bike_sharing$weathersit)
# Tranforms season in factor to facilitate its usage as it's a categorical variable
bike_sharing$season <- as.factor(bike_sharing$season)
#Barplot weather situation per day
ggplot(data = bike_sharing, aes(x = season)) +
geom_bar() +
labs(x = "Season", , title = "Chart 11: Quantity of days for each season")
#Create your contingency table
mytable<-xtabs(~ubullsch+rsex, data=bully)
xtabs(~weathersit+season, data=bike_sharing)
stats::chisq.test(mytable, correct=TRUE)
#Create your contingency table
mytable<-xtabs(~weathersit+season, data=bike_sharing)
mytable %>%
kbl(caption = "Table 6: Descriptive statistics of bikes hired per weekday") %>%
kable_styling()
?xtabs
as.data.frame.matrix(mytable) %>%
kbl(caption = "Table 6: Descriptive statistics of bikes hired per weekday") %>%
kable_styling()
mytableDf <- as.data.frame.matrix(mytable)
data.frame(Weather = c(1,2,3), mytableDf, row.names = NULL)
data.frame(Weather = c(1,2,3), mytableDf, row.names = c(1,2,3))
data.frame(mytableDf, row.names = c("Weather 1","Weather 2","Weather 3"))
data.frame(mytableDf, row.names = c("Weather 1","Weather 2","Weather 3"), col.names = c(1,2,3,4))
data.frame(mytableDf, row.names = c("Weather 1","Weather 2","Weather 3"), colnames = c(1,2,3,4))
data.frame(mytableDf, row.names = c("Weather 1","Weather 2","Weather 3"))
data.frame(mytableDf, row.names = c("Weather 1","Weather 2","Weather 3"), colnames = c(1,2,3,4,5))
data.frame(mytableDf, row.names = c("Weather 1","Weather 2","Weather 3"), colnames = c(1,2,3))
?data.frame
mytableDf <- as.data.frame.matrix(mytable)[c(1,2,3,4),]
data.frame(mytableDf, row.names = c("Weather 1","Weather 2","Weather 3"))
mytableDf <- as.data.frame.matrix(mytable)[c(1,2,3),]
data.frame(mytableDf, row.names = c("Weather 1","Weather 2","Weather 3"))
mytableDf <- as.data.frame.matrix(mytable)[,c(1,2,3)]
data.frame(mytableDf, row.names = c("Weather 1","Weather 2","Weather 3"))
mytableDf <- as.data.frame.matrix(mytable)
data.frame(mytableDf, row.names = c("Weather 1","Weather 2","Weather 3"))
#transforms xtabs into dataframe to show as table
mytableDfMat <- as.data.frame.matrix(mytable)
mytableDf <- data.frame(mytableDfMat, row.names = c("Weather 1","Weather 2","Weather 3"))
colnames(mytableDf) <- c(1,2,3,4)
mytableDf %>%
kbl(caption = "Table 6: Descriptive statistics of bikes hired per weekday") %>%
kable_styling()
#transforms xtabs into dataframe and rename rows and columnsto show as table
mytableDfMat <- as.data.frame.matrix(mytable)
mytableDf <- data.frame(mytableDfMat, row.names = c("Weather 1","Weather 2","Weather 3"))
colnames(mytableDf) <- c("Season 1","Season 2","Season 3","Season 4")
mytableDf %>%
kbl(caption = "Table 6: Descriptive statistics of bikes hired per weekday") %>%
kable_styling()
mytableDf %>%
kbl(caption = "Table 7: Days with a specific weather per season") %>%
kable_styling()
#chi square test
#correct=TRUE to get Yates correction, this is needed because this table has cells with a count smaller than 5
ctest<-stats::chisq.test(mytable, correct=TRUE)
ctest$p.value
#Create your contingency table
mytable<-xtabs(~weathersit+season, data=bike_sharing)
#transforms xtabs into dataframe and rename rows and columnsto show as table
mytableDfMat <- as.data.frame.matrix(mytable)
mytableDf <- data.frame(mytableDfMat, row.names = c("Weather 1","Weather 2","Weather 3"))
colnames(mytableDf) <- c("Season 1","Season 2","Season 3","Season 4")
mytableDf %>%
kbl(caption = "Table 7: Days with a specific weather per season") %>%
kable_styling()
#chi square test
#correct=TRUE to get Yates correction, this is needed because this table has cells with a count smaller than 5
ctest<-stats::chisq.test(mytable, correct=TRUE)
ctest$p.value
#Create your contingency table
mytable<-xtabs(~weathersit+season, data=bike_sharing)
#chi square test
#correct=TRUE to get Yates correction, this is needed because this table has cells with a count smaller than 5
ctest<-stats::chisq.test(mytable, correct=TRUE)
ctest$p.value
ctest$expected#expected frequencies
ctest$observed
sjstats::phi(mytable)
sjstats::cramer(mytable)
ctest$expected#expected frequencies
ctest$expected#expected frequencies %>%
ctest$expected %>% #expected frequencies
kbl(caption = "Table 8: Expected Frequencies") %>%
kable_styling()
#transforms xtabs into dataframe and rename rows and columns to show as table
xtabsToDataframe <- function(mytable, rownamesParm, colnamesParm) {
mytableDfMat <- as.data.frame.matrix(mytable)
mytableDf <- data.frame(mytableDfMat, row.names = rownamesParm)
colnames(mytableDf) <- colnamesParm
return(mytableDf)
}
weatherNames = c("Weather 1","Weather 2","Weather 3")
seasonNames = c("Season 1","Season 2","Season 3","Season 4")
#Create your contingency table
mytable<-xtabs(~weathersit+season, data=bike_sharing)
xtabsToDataframe(mytable, weatherNames, seasonNames)
#transforms xtabs into dataframe and rename rows and columns to show as table
xtabsToDataframe <- function(aXtabs, rownamesParm, colnamesParm) {
mytableDfMat <- as.data.frame.matrix(aXtabs)
mytableDf <- data.frame(mytableDfMat, row.names = rownamesParm)
colnames(mytableDf) <- colnamesParm
return(mytableDf)
}
weatherNames = c("Weather 1","Weather 2","Weather 3")
seasonNames = c("Season 1","Season 2","Season 3","Season 4")
#Create your contingency table
mytable<-xtabs(~weathersit+season, data=bike_sharing)
xtabsToDataframe(mytable, weatherNames, seasonNames)
xtabsToDataframe(mytable, weatherNames, seasonNames) %>%
kbl(caption = "Table 7: Days with a specific weather per season") %>%
kable_styling()
xtabsToDataframe(ctest$expected, weatherNames, seasonNames) %>% #expected frequencies
kbl(caption = "Table 8: Expected Frequencies") %>%
kable_styling()
#Using Cramer to access the effect size since this is a 3x4 contingency table, not a 2x2
sjstats::cramer(mytable)
#chi square test
#correct=TRUE to get Yates correction, this is needed because this table has cells with a count smaller than 5
ctest<-stats::chisq.test(mytable, correct=TRUE)
xtabsToDataframe(ctest$expected, weatherNames, seasonNames) %>% #expected frequencies
kbl(caption = "Table 8: Expected Frequencies") %>%
kable_styling()
xtabsToDataframe(ctest$observed, weatherNames, seasonNames) %>%#observed frequencies
kbl(caption = "Table 9: Observed Frequencies") %>%
kable_styling()
#Using Cramer to access the effect size since this is a 3x4 contingency table, not a 2x2
sjstats::cramer(mytable)
sjstats::cramer(mytable)
sjstats::cramer(mytable)[[1]]
#Using Cramer to access the effect size since this is a 3x4 contingency table, not a 2x2
creamerEffctSize <- sjstats::cramer(mytable)
stats::chisq.test(mytable, correct=TRUE)
ctest$parameter
ctest$parameter[1]
ctest$parameter[[1]]
sjstats::cramer(mytable)
#Using Cramer to access the effect size since this is a 3x4 contingency table, not a 2x2
creamerEffctSize <- round(sjstats::cramer(mytable),2)
round(sjstats::cramer(mytable),2)
